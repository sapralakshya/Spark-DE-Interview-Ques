{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d13076",
   "metadata": {},
   "source": [
    "# 1.) Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark=SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "numberrdd=sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47433a51",
   "metadata": {},
   "source": [
    "b) Implement transformations and actions on the RDD to perform data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsrdd=sc.parallelize(['hello','hi','hello'])\n",
    "newrdd=wordsrdd.map(lambda x: (x,1)).groupByKey(lambda a,b:a+b)\n",
    "print(newrdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb9ed4",
   "metadata": {},
   "source": [
    "c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter transformation:\n",
    "filterrdd=numberrdd.filter(lambda x: x%2==0)\n",
    "\n",
    "#map transformation:      \n",
    "wordsrdd=sc.parallelize(['hello','hi','hello'])\n",
    "newrdd=wordsrdd.map(lambda x: (x,1)).groupByKey(lambda a,b:a+b)\n",
    "print(newrdd.collect())\n",
    "\n",
    "#reduce transformation:\n",
    "no_of_char=wordsrdd.map(lambda x:(len(x))).reduce(lambda a,b:a+b)\n",
    "\n",
    "#aggregate transformation:\n",
    "numberrdd=sc.parallelize([1,2,3,4,5])\n",
    "sum_of_num=numberrdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16999d",
   "metadata": {},
   "source": [
    "# 2.)Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Dataframe_practice\").getOrCreate()\n",
    "Employeesdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Employees.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e276e8a",
   "metadata": {},
   "source": [
    " b)Perform common DataFrame operations such as filtering, grouping, or joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2238292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining operation:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Dataframe_practice\").getOrCreate()\n",
    "Employeesdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Employees.txt')\n",
    "Departmentdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Department.txt')\n",
    "Employeesdf.join(Departmentdf,Employeesdf.DEPARTMENT_ID == Departmentdf.DEPARTMENT_ID,\"inner\").select(Employeesdf.EMPLOYEE_ID,Employeesdf.FIRST_NAME,Departmentdf.DEPARTMENT_ID).show(10)\n",
    "\n",
    "\n",
    "#Grouping operation:\n",
    "spark=SparkSession.builder.appName(\"Dataframe_practice\").getOrCreate()\n",
    "Employeesdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Employees.txt')\n",
    "Departmentdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Department.txt')\n",
    "groupeddf=Employeesdf.groupBy(col(\"DEPARTMENT_ID\")).agg(SUM(\"SALARY\").alias(\"TOTAL_SALARY\"))\n",
    "\n",
    "#Filtering operation:\n",
    "spark=SparkSession.builder.appName(\"Dataframe_practice\").getOrCreate()\n",
    "Employeesdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Employees.txt')\n",
    "Departmentdf=spark.read.option('Header',True).option('InferSchema',True).csv('/spark_sample/Department.txt')\n",
    "Filterddf=Employeesdf.filter('DEPARTMENT_ID==10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9884b",
   "metadata": {},
   "source": [
    "c) Apply Spark SQL queries on the DataFrame to extract insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59551262",
   "metadata": {},
   "outputs": [],
   "source": [
    "Employeedf.createOrReplaceTempView(\"Employees\")\n",
    "spark.sql('Select Employee_name,Salary from Employees where department_id=10 order by salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631135a1",
   "metadata": {},
   "source": [
    "# 3.) Spark Streaming:\n",
    "   a) Write a Python program to create a Spark Streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"SparkStreamingApp\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420b161",
   "metadata": {},
   "source": [
    "b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9041037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "sc = SparkContext(\"local[2]\", \"SparkStreamingApp\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "kafka_params = {\n",
    "\"bootstrap.servers\": \"localhost:9092\",  # Kafka broker(s) address\n",
    "\"group.id\": \"spark-streaming-app\",  # Consumer group ID\n",
    "\"auto.offset.reset\": \"latest\"  # Start reading from the latest offset\n",
    "}\n",
    "\n",
    "kafka_stream = KafkaUtils.createDirectStream(ssc, [\"test-topic\"], kafka_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bb2f1",
   "metadata": {},
   "source": [
    "c) Implement streaming transformations and actions to process and analyze the incoming data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7248fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = kafka_stream.map(lambda x: x[1])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "word_counts.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1f0a8",
   "metadata": {},
   "source": [
    "# 4.) Spark SQL and Data Source Integration:\n",
    "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c11406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"SparkDatabaseConnection\") \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/path/to/jdbc/driver.jar\") \\\n",
    ".getOrCreate()\n",
    "database_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "database_properties = {\n",
    "          \"user\": \"myuser\",\n",
    "          \"password\": \"mypassword\",\n",
    "          \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "df = spark.read.jdbc(url=database_url, table=\"mytable\", properties=database_properties)\n",
    "df.show()\n",
    "df.write.jdbc(url=database_url, table=\"newtable\", mode=\"overwrite\", properties=database_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f048f",
   "metadata": {},
   "source": [
    "b)Perform SQL operations on the data stored in the database using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql('select * from data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c503b",
   "metadata": {},
   "source": [
    "c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78d3de",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "Spark can read data from HDFS using the spark.read API. It supports various file formats such as Parquet, Avro, JSON, CSV, and  more.\n",
    "You can specify the HDFS file path when reading data, like spark.read.parquet(\"hdfs://localhost:9000/path/to/file.parquet\").\n",
    "Spark automatically parallelizes the data loading process, allowing for efficient distributed processing of large datasets.\n",
    "\n",
    "##### Writing Data to HDFS:\n",
    "\n",
    "Spark provides the DataFrame.write API to write data to HDFS. It supports various file formats and partitioning options.\n",
    "You can save a DataFrame to HDFS using df.write.parquet(\"hdfs://localhost:9000/path/to/output.parquet\"), specifying the desired file format.\n",
    "Partitioning data while writing allows for better data organization and improves query performance.\n",
    "\n",
    "##### Reading Data from S3:\n",
    "\n",
    "Spark can read data from S3 using the spark.read API. It supports various file formats such as Parquet, Avro, JSON, CSV, and more.\n",
    "You can specify the S3 bucket and file path when reading data, like spark.read.parquet(\"s3a://bucket-name/path/to/file.parquet\").\n",
    "Spark can automatically parallelize the data loading process, enabling distributed processing of large datasets stored in S3. \n",
    "\n",
    "##### Writing Data to S3:\n",
    "\n",
    "Spark provides the DataFrame.write API to write data to S3. It supports various file formats and partitioning options.\n",
    "You can save a DataFrame to S3 using df.write.parquet(\"s3a://bucket-name/path/to/output.parquet\"), specifying the desired file format.\n",
    "Partitioning data while writing allows for better data organization and improves query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856c104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
